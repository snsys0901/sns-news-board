# -*- coding: utf-8 -*-
"""
ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ (íŒŒë‚˜ì‹œì•„ í¬í•¨, ìµœì¢…)
- Requests ê¸°ë°˜ NewsAPI, Google RSS, Naver í¬ë¡¤ë§
- TFâ€“IDF í‚¤ì›Œë“œ ì¶”ì¶œ
- Streamlit ìºì‹œ ì²˜ë¦¬
- ìŠ¤ì¼€ì¤„ëŸ¬(ë©”ì¼ ë°œì†¡) ìœ ì§€
"""

import os
import re
import json
import time
import hashlib
import logging
import threading
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import List, Dict, Optional

import requests
import pandas as pd
import streamlit as st
import altair as alt
from bs4 import BeautifulSoup
from feedparser import parse as rss_parse
from sklearn.feature_extraction.text import TfidfVectorizer
import schedule
import smtplib
from email.mime.text import MIMEText

# -----------------------------
# ë¡œê¹… ì„¤ì •
# -----------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger("NewsBoard")

# -----------------------------
# ì „ì—­ ì„¤ì •
# -----------------------------
CACHE_FILE = Path.home() / ".news_cache.json"
QUERIES = {
    "ì—ìŠ¤ì—”ì‹œìŠ¤": "ì—ìŠ¤ì—”ì‹œìŠ¤ OR S&SYS",
    "ì‚¼ì„±ì¤‘ê³µì—…": "ì‚¼ì„±ì¤‘ê³µì—…",
    "í•œí™”ì˜¤ì…˜": "í•œí™”ì˜¤ì…˜",
    "í•œë¼IMS": "í•œë¼IMS",
    "íŒŒë‚˜ì‹œì•„": "íŒŒë‚˜ì‹œì•„"
}
KEYWORDS = {
    "ì—ìŠ¤ì—”ì‹œìŠ¤": ["ì—ìŠ¤ì—”ì‹œìŠ¤", "s&sys"],
    "ì‚¼ì„±ì¤‘ê³µì—…": ["ì‚¼ì„±ì¤‘ê³µì—…"],
    "í•œí™”ì˜¤ì…˜": ["í•œí™”ì˜¤ì…˜"],
    "í•œë¼IMS": ["í•œë¼ims"],
    "íŒŒë‚˜ì‹œì•„": ["íŒŒë‚˜ì‹œì•„"]
}

NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")
SMTP_SERVER   = os.getenv("SMTP_SERVER",   "smtp.example.com")
SMTP_PORT     = int(os.getenv("SMTP_PORT",  587))
SMTP_USER     = os.getenv("SMTP_USER",     "user@example.com")
SMTP_PASSWORD = os.getenv("SMTP_PASSWORD", "password")

# -----------------------------
# ìœ í‹¸ í•¨ìˆ˜
# -----------------------------
def _shorten(text: str, width: int = 60) -> str:
    return text if len(text) <= width else text[:width] + "â€¦"

def parse_datetime_naive(s: str) -> Optional[datetime]:
    for fmt in ("%Y-%m-%d %H:%M", "%Y-%m-%dT%H:%M:%SZ",
                "%a, %d %b %Y %H:%M:%S %z", "%Y-%m-%d"):
        try:
            return datetime.strptime(s, fmt)
        except:
            continue
    return None

def extract_top_keywords(docs: List[str], top_n: int = 5) -> List[str]:
    docs = [d for d in docs if d.strip()]
    if not docs:
        return []
    vect = TfidfVectorizer(ngram_range=(1,2), max_features=100)
    X = vect.fit_transform(docs)
    scores = X.sum(axis=0).A1
    terms  = vect.get_feature_names_out()
    top_idx = scores.argsort()[::-1][:top_n]
    return [terms[i] for i in top_idx]

# -----------------------------
# ìºì‹œ ë¡œë“œ/ì—…ë°ì´íŠ¸
# -----------------------------
def _load_cache() -> Dict[str, Dict]:
    try:
        if CACHE_FILE.exists():
            return json.loads(CACHE_FILE.read_text("utf-8"))
    except:
        logger.exception("ìºì‹œ ë¡œë“œ ì‹¤íŒ¨, ì´ˆê¸°í™”")
    return {}

def update_cache(arts: List[Dict]) -> None:
    cache = _load_cache()
    changed = False
    for a in arts:
        url = a.get("url","")
        if not url: continue
        uid = hashlib.sha256(url.encode()).hexdigest()
        if uid not in cache:
            cache[uid] = a; changed = True
        else:
            prev = cache[uid]
            orgs = prev.get("origins",[])
            new_orgs = list(dict.fromkeys(orgs + a.get("origins",[])))
            if new_orgs != orgs:
                cache[uid]["origins"] = new_orgs; changed = True
            new_pa = a.get("publishedAt")
            if new_pa and new_pa != prev.get("publishedAt"):
                cache[uid]["publishedAt"] = new_pa; changed = True
    if changed:
        try:
            CACHE_FILE.write_text(json.dumps(cache, ensure_ascii=False, indent=2), "utf-8")
            logger.info("ìºì‹œ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        except:
            logger.exception("ìºì‹œ ì €ì¥ ì‹¤íŒ¨")

# -----------------------------
# ë™ê¸°ì‹ ë‰´ìŠ¤ ìˆ˜ì§‘
# -----------------------------
@st.cache_data(ttl=3600)
def fetch_newsapi_sync(q: str) -> List[Dict]:
    if not NEWS_API_KEY:
        return []
    try:
        since = (datetime.utcnow() - timedelta(days=2)).strftime("%Y-%m-%dT%H:%M:%SZ")
        r = requests.get("https://newsapi.org/v2/everything", params={
            "q":q, "language":"ko", "sortBy":"publishedAt",
            "from":since, "apiKey":NEWS_API_KEY, "pageSize":50
        }, timeout=15)
        r.raise_for_status()
        arts = r.json().get("articles",[])
        for a in arts:
            a.setdefault("origins",[]).append("newsapi")
            a.setdefault("source",{})["name"] = a.get("source",{}).get("name","NewsAPI")
        return arts
    except:
        logger.exception("NewsAPI ìˆ˜ì§‘ ì˜¤ë¥˜")
        return []

@st.cache_data(ttl=3600)
def fetch_rss_sync(q: str) -> List[Dict]:
    out=[]; seen=set()
    for term in re.split(r"\s+OR\s+", q):
        url = f"https://news.google.com/rss/search?q={requests.utils.quote(term)}&hl=ko&gl=KR&ceid=KR:ko"
        try:
            r = requests.get(url, timeout=15); r.raise_for_status()
            feed = rss_parse(r.text)
            for e in feed.entries:
                if e.link in seen: continue
                seen.add(e.link)
                dt = time.strftime("%Y-%m-%d %H:%M", e.published_parsed) if getattr(e,"published_parsed",None) else e.get("published","")
                out.append({"title":e.title,"url":e.link,"publishedAt":dt,"source":{"name":"Google RSS"},"origins":["rss"]})
        except:
            logger.warning(f"RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {url}")
    return out

@st.cache_data(ttl=3600)
def fetch_naver(q: str) -> List[Dict]:
    try:
        r = requests.get(
            f"https://search.naver.com/search.naver?where=news&query={requests.utils.quote(q)}&sort=1",
            headers={"User-Agent":"Mozilla/5.0"},
            timeout=15
        ); r.raise_for_status()
        soup=BeautifulSoup(r.text,"html.parser"); out=[]
        for li in soup.select("li.bx"):
            tag=li.select_one("a.news_tit")
            if not tag: continue
            raw = li.select_one("span.info").get_text(strip=True) if li.select_one("span.info") else ""
            out.append({"title":tag["title"].strip(),"url":tag["href"].strip(),
                        "publishedAt":raw,"source":{"name":"Naver"},"origins":["naver"]})
        return out
    except:
        logger.exception("Naver ìˆ˜ì§‘ ì˜¤ë¥˜")
        return []

def fetch_news(q: str, mode: str, use_naver: bool) -> List[Dict]:
    arts=[]
    if mode!="RSSë§Œ": arts+=fetch_newsapi_sync(q)
    if mode!="NewsAPIë§Œ": arts+=fetch_rss_sync(q)
    if use_naver:      arts+=fetch_naver(q)
    update_cache(arts)
    return arts

# -----------------------------
# ì´ë©”ì¼ ê¸°ëŠ¥
# -----------------------------
def generate_email_content(data: Dict[str,List[Dict]]) -> str:
    lines=[]
    for comp, arts in data.items():
        lines.append(f"## {comp}\n")
        for a in sorted(arts, key=lambda x: parse_datetime_naive(x["publishedAt"]) or datetime.min, reverse=True)[:10]:
            ts=parse_datetime_naive(a["publishedAt"])
            ts=ts.strftime("%Y-%m-%d %H:%M") if ts else "ë¯¸í™•ì¸"
            lines.append(f"- {a['title']} ({ts})\n  {a['url']}\n")
        lines.append("\n")
    return "".join(lines)

def send_email(subject: str, content: str, tos: List[str]) -> None:
    if not tos: return
    try:
        msg=MIMEText(content,_charset="utf-8")
        msg["Subject"]=subject; msg["From"]=SMTP_USER; msg["To"]=", ".join(tos)
        with smtplib.SMTP(SMTP_SERVER,SMTP_PORT) as smtp:
            smtp.starttls()
            smtp.login(SMTP_USER,SMTP_PASSWORD)
            smtp.sendmail(SMTP_USER,tos,msg.as_string())
        logger.info(f"ì´ë©”ì¼ ë°œì†¡: {tos}")
    except:
        logger.exception("ì´ë©”ì¼ ë°œì†¡ ì˜¤ë¥˜")

def send_daily_email() -> None:
    data={comp:fetch_news(q,mode="ì „ì²´ (ë„¤ì´ë²„ í¬í•¨)",use_naver=True) for comp,q in QUERIES.items()}
    content=generate_email_content(data)
    tos=[st.session_state.get(f"email{i}") for i in range(1,6) if st.session_state.get(f"email{i}")]
    send_email("ë‰´ìŠ¤ ë³´ë“œ ìš”ì•½",content,tos)

# -----------------------------
# Streamlit UI
# -----------------------------
def render_articles(comp: str, arts: List[Dict], cnt: int) -> None:
    st.subheader(f"ğŸ“° {comp} ë‰´ìŠ¤")
    if not arts:
        st.info("ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    for a in sorted(arts,key=lambda x: parse_datetime_naive(x["publishedAt"]) or datetime.min,reverse=True)[:cnt]:
        ts=parse_datetime_naive(a["publishedAt"]) or datetime.min
        ts=ts.strftime("%Y-%m-%d %H:%M")
        st.markdown(f"- [{_shorten(a['title'])}]({a['url']}) ({ts})")

def analyze_trends(src: str) -> None:
    st.markdown("---")
    st.header(f"ğŸ“ˆ ë…¸ì¶œ ì´ë ¥ (31ì¼) â€” {src}")
    cache=_load_cache()
    today=date.today()
    dates=[(today-timedelta(days=i)).strftime("%Y-%m-%d") for i in reversed(range(31))]
    cmap={d:{c:0 for c in KEYWORDS} for d in dates}
    for itm in cache.values():
        dt=parse_datetime_naive(itm["publishedAt"])
        if not dt: continue
        d0=dt.strftime("%Y-%m-%d")
        if d0 not in cmap: continue
        tl=itm["title"].lower()
        for comp,kws in KEYWORDS.items():
            if any(kw in tl for kw in kws):
                cmap[d0][comp]+=1
    rec=[{"date":d,"company":c,"count":cmap[d][c]} for d in dates for c in KEYWORDS]
    df=pd.DataFrame(rec)
    df["date_fmt"]=pd.to_datetime(df["date"]).dt.strftime("%m-%d")
    chart=(alt.Chart(df).mark_bar()
           .encode(x=alt.X("date_fmt:N",title="ë‚ ì§œ"),
                   y=alt.Y("count:Q",title="ê±´ìˆ˜"),
                   color=alt.Color("company:N",title="íšŒì‚¬"),
                   tooltip=["date_fmt","company","count"])
           .properties(width="container",height=300))
    st.altair_chart(chart,use_container_width=True)

def main() -> None:
    st.set_page_config(page_title="ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ë³´ë“œ",layout="wide")
    st.title("ğŸ“— ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ë³´ë“œ")

    # â”€ ì‚¬ì´ë“œë°” â”€
    src=st.sidebar.selectbox("ë‰´ìŠ¤ ì†ŒìŠ¤",
        ["ì „ì²´ (ë„¤ì´ë²„ í¬í•¨)","ì „ì²´ (ë„¤ì´ë²„ ì œì™¸)","RSSë§Œ","NewsAPIë§Œ"])
    use_nv="í¬í•¨" in src
    cnt=st.sidebar.selectbox("í‘œì‹œ ë‰´ìŠ¤ ê±´ìˆ˜",[5,10,12,15],index=1)

    st.sidebar.header("ğŸ“§ ë©”ì¼ ì„¤ì •")
    t=st.sidebar.selectbox("ë©”ì¼ ë°œì†¡ ì‹œê°„",["08:00","10:00","14:00","16:00"],key="send_time")
    for i in range(1,6):
        st.sidebar.text_input(f"ì´ë©”ì¼ {i}",key=f"email{i}")
    if st.sidebar.button("ë©”ì¼ ìŠ¤ì¼€ì¤„ ì €ì¥"):
        schedule.clear("emailjob")
        schedule.every().day.at(t).do(send_daily_email).tag("emailjob")
        st.sidebar.success("ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

    st.sidebar.header("ë…¸ì¶œ ì´ë ¥ ì„¤ì •")
    ts=st.sidebar.selectbox("ì†ŒìŠ¤ ì„ íƒ",["ì „ì²´","ë„¤ì´ë²„ë§Œ","êµ¬ê¸€ RSSë§Œ"],index=0)

    # ìŠ¤ì¼€ì¤„ëŸ¬
    if "sched" not in st.session_state:
        def _run():
            while True:
                schedule.run_pending(); time.sleep(1)
        threading.Thread(target=_run,daemon=True).start()
        st.session_state["sched"]=True

    if st.sidebar.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()
        st.rerun()

    # â”€ íƒ­ë³„ ë‰´ìŠ¤ â”€
    tabs=st.tabs(list(QUERIES.keys()))
    for tab,comp in zip(tabs,QUERIES):
        with tab:
            arts=fetch_news(QUERIES[comp],src,use_nv)
            render_articles(comp,arts,cnt)

    # â”€ í‚¤ì›Œë“œ ë¶„ì„ â”€
    st.markdown("---"); st.header("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„")
    cols=st.columns(len(QUERIES))
    for col,comp in zip(cols,QUERIES):
        with col:
            arts=fetch_news(QUERIES[comp],src,use_nv)
            titles=[a["title"] for a in sorted(arts,key=lambda x: parse_datetime_naive(x["publishedAt"]) or datetime.min,reverse=True)[:cnt]]
            kws=extract_top_keywords(titles,5)
            if kws:
                for kw in kws: st.markdown(f"- {kw}")
            else:
                st.write("í‚¤ì›Œë“œ ì—†ìŒ")

    # â”€ ë…¸ì¶œ ì´ë ¥ â”€
    analyze_trends(ts)

if __name__ == "__main__":
    main()
