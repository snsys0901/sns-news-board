# -*- coding: utf-8 -*-
"""
ÏóêÏä§ÏóîÏãúÏä§ Îâ¥Ïä§ ÎåÄÏãúÎ≥¥Îìú (ÌååÎÇòÏãúÏïÑ Ìè¨Ìï®, ÏµúÏ¢Ö)
- Requests Í∏∞Î∞ò NewsAPI, Google RSS, Naver ÌÅ¨Î°§ÎßÅ
- TF‚ÄìIDF ÌÇ§ÏõåÎìú Ï∂îÏ∂ú
- Streamlit Ï∫êÏãú Ï≤òÎ¶¨
- Ïä§ÏºÄÏ§ÑÎü¨(Î©îÏùº Î∞úÏÜ°) Ïú†ÏßÄ
"""

import os
import re
import json
import time
import hashlib
import logging
import threading
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import List, Dict, Optional

import requests
import pandas as pd
import streamlit as st
import altair as alt
from bs4 import BeautifulSoup
from feedparser import parse as rss_parse
from sklearn.feature_extraction.text import TfidfVectorizer
import schedule
import smtplib
from email.mime.text import MIMEText

# -----------------------------
# Î°úÍπÖ ÏÑ§Ï†ï
# -----------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger("NewsBoard")

# -----------------------------
# Ï†ÑÏó≠ ÏÑ§Ï†ï
# -----------------------------
CACHE_FILE = Path.home() / ".news_cache.json"
QUERIES = {
    "ÏóêÏä§ÏóîÏãúÏä§": "ÏóêÏä§ÏóîÏãúÏä§ OR S&SYS",
    "ÏÇºÏÑ±Ï§ëÍ≥µÏóÖ": "ÏÇºÏÑ±Ï§ëÍ≥µÏóÖ",
    "ÌïúÌôîÏò§ÏÖò": "ÌïúÌôîÏò§ÏÖò",
    "ÌïúÎùºIMS": "ÌïúÎùºIMS",
    "ÌååÎÇòÏãúÏïÑ": "ÌååÎÇòÏãúÏïÑ"
}
KEYWORDS = {
    "ÏóêÏä§ÏóîÏãúÏä§": ["ÏóêÏä§ÏóîÏãúÏä§", "s&sys"],
    "ÏÇºÏÑ±Ï§ëÍ≥µÏóÖ": ["ÏÇºÏÑ±Ï§ëÍ≥µÏóÖ"],
    "ÌïúÌôîÏò§ÏÖò": ["ÌïúÌôîÏò§ÏÖò"],
    "ÌïúÎùºIMS": ["ÌïúÎùºims"],
    "ÌååÎÇòÏãúÏïÑ": ["ÌååÎÇòÏãúÏïÑ"]
}

NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")
SMTP_SERVER   = os.getenv("SMTP_SERVER",   "smtp.example.com")
SMTP_PORT     = int(os.getenv("SMTP_PORT",  587))
SMTP_USER     = os.getenv("SMTP_USER",     "user@example.com")
SMTP_PASSWORD = os.getenv("SMTP_PASSWORD", "password")

# -----------------------------
# Ïú†Ìã∏ Ìï®Ïàò
# -----------------------------
def _shorten(text: str, width: int = 60) -> str:
    return text if len(text) <= width else text[:width] + "‚Ä¶"

def parse_datetime_naive(s: str) -> Optional[datetime]:
    for fmt in ("%Y-%m-%d %H:%M", "%Y-%m-%dT%H:%M:%SZ",
                "%a, %d %b %Y %H:%M:%S %z", "%Y-%m-%d"):
        try:
            return datetime.strptime(s, fmt)
        except:
            continue
    return None

def extract_top_keywords(docs: List[str], top_n: int = 5) -> List[str]:
    docs = [d for d in docs if d.strip()]
    if not docs:
        return []
    vect = TfidfVectorizer(ngram_range=(1,2), max_features=100)
    X = vect.fit_transform(docs)
    scores = X.sum(axis=0).A1
    terms  = vect.get_feature_names_out()
    top_idx = scores.argsort()[::-1][:top_n]
    return [terms[i] for i in top_idx]

# -----------------------------
# Ï∫êÏãú Î°úÎìú/ÏóÖÎç∞Ïù¥Ìä∏
# -----------------------------
def _load_cache() -> Dict[str, Dict]:
    try:
        if CACHE_FILE.exists():
            return json.loads(CACHE_FILE.read_text("utf-8"))
    except:
        logger.exception("Ï∫êÏãú Î°úÎìú Ïã§Ìå®, Ï¥àÍ∏∞Ìôî")
    return {}

def update_cache(arts: List[Dict]) -> None:
    cache = _load_cache()
    changed = False
    for a in arts:
        url = a.get("url","")
        if not url: continue
        uid = hashlib.sha256(url.encode()).hexdigest()
        if uid not in cache:
            cache[uid] = a; changed = True
        else:
            prev = cache[uid]
            orgs = prev.get("origins",[])
            new_orgs = list(dict.fromkeys(orgs + a.get("origins",[])))
            if new_orgs != orgs:
                cache[uid]["origins"] = new_orgs; changed = True
            new_pa = a.get("publishedAt")
            if new_pa and new_pa != prev.get("publishedAt"):
                cache[uid]["publishedAt"] = new_pa; changed = True
    if changed:
        try:
            CACHE_FILE.write_text(json.dumps(cache, ensure_ascii=False, indent=2), "utf-8")
            logger.info("Ï∫êÏãú ÏóÖÎç∞Ïù¥Ìä∏ ÏôÑÎ£å")
        except:
            logger.exception("Ï∫êÏãú Ï†ÄÏû• Ïã§Ìå®")

# -----------------------------
# ÎèôÍ∏∞Ïãù Îâ¥Ïä§ ÏàòÏßë
# -----------------------------
@st.cache_data(ttl=3600)
def fetch_newsapi_sync(q: str) -> List[Dict]:
    if not NEWS_API_KEY:
        return []
    try:
        since = (datetime.utcnow() - timedelta(days=2)).strftime("%Y-%m-%dT%H:%M:%SZ")
        r = requests.get("https://newsapi.org/v2/everything", params={
            "q":q, "language":"ko", "sortBy":"publishedAt",
            "from":since, "apiKey":NEWS_API_KEY, "pageSize":50
        }, timeout=15)
        r.raise_for_status()
        arts = r.json().get("articles",[])
        for a in arts:
            a.setdefault("origins",[]).append("newsapi")
            a.setdefault("source",{})["name"] = a.get("source",{}).get("name","NewsAPI")
        return arts
    except:
        logger.exception("NewsAPI ÏàòÏßë Ïò§Î•ò")
        return []

@st.cache_data(ttl=3600)
def fetch_rss_sync(q: str) -> List[Dict]:
    out=[]; seen=set()
    for term in re.split(r"\s+OR\s+", q):
        url = f"https://news.google.com/rss/search?q={requests.utils.quote(term)}&hl=ko&gl=KR&ceid=KR:ko"
        try:
            r = requests.get(url, timeout=15); r.raise_for_status()
            feed = rss_parse(r.text)
            for e in feed.entries:
                if e.link in seen: continue
                seen.add(e.link)
                dt = time.strftime("%Y-%m-%d %H:%M", e.published_parsed) if getattr(e,"published_parsed",None) else e.get("published","")
                out.append({"title":e.title,"url":e.link,"publishedAt":dt,"source":{"name":"Google RSS"},"origins":["rss"]})
        except:
            logger.warning(f"RSS ÏàòÏßë Ïã§Ìå®: {url}")
    return out

@st.cache_data(ttl=3600)
def fetch_naver(q: str) -> List[Dict]:
    try:
        r = requests.get(
            f"https://search.naver.com/search.naver?where=news&query={requests.utils.quote(q)}&sort=1",
            headers={"User-Agent":"Mozilla/5.0"},
            timeout=15
        ); r.raise_for_status()
        soup=BeautifulSoup(r.text,"html.parser"); out=[]
        for li in soup.select("li.bx"):
            tag=li.select_one("a.news_tit")
            if not tag: continue
            raw = li.select_one("span.info").get_text(strip=True) if li.select_one("span.info") else ""
            out.append({"title":tag["title"].strip(),"url":tag["href"].strip(),
                        "publishedAt":raw,"source":{"name":"Naver"},"origins":["naver"]})
        return out
    except:
        logger.exception("Naver ÏàòÏßë Ïò§Î•ò")
        return []

def fetch_news(q: str, mode: str, use_naver: bool) -> List[Dict]:
    arts=[]
    if mode!="RSSÎßå": arts+=fetch_newsapi_sync(q)
    if mode!="NewsAPIÎßå": arts+=fetch_rss_sync(q)
    if use_naver:      arts+=fetch_naver(q)
    update_cache(arts)
    return arts

# -----------------------------
# Ïù¥Î©îÏùº Í∏∞Îä•
# -----------------------------
def generate_email_content(data: Dict[str,List[Dict]]) -> str:
    lines=[]
    for comp, arts in data.items():
        lines.append(f"## {comp}\n")
        for a in sorted(arts, key=lambda x: parse_datetime_naive(x["publishedAt"]) or datetime.min, reverse=True)[:10]:
            ts=parse_datetime_naive(a["publishedAt"])
            ts=ts.strftime("%Y-%m-%d %H:%M") if ts else "ÎØ∏ÌôïÏù∏"
            lines.append(f"- {a['title']} ({ts})\n  {a['url']}\n")
        lines.append("\n")
    return "".join(lines)

def send_email(subject: str, content: str, tos: List[str]) -> None:
    if not tos: return
    try:
        msg=MIMEText(content,_charset="utf-8")
        msg["Subject"]=subject; msg["From"]=SMTP_USER; msg["To"]=", ".join(tos)
        with smtplib.SMTP(SMTP_SERVER,SMTP_PORT) as smtp:
            smtp.starttls()
            smtp.login(SMTP_USER,SMTP_PASSWORD)
            smtp.sendmail(SMTP_USER,tos,msg.as_string())
        logger.info(f"Ïù¥Î©îÏùº Î∞úÏÜ°: {tos}")
    except:
        logger.exception("Ïù¥Î©îÏùº Î∞úÏÜ° Ïò§Î•ò")

def send_daily_email() -> None:
    data={comp:fetch_news(q,mode="Ï†ÑÏ≤¥ (ÎÑ§Ïù¥Î≤Ñ Ìè¨Ìï®)",use_naver=True) for comp,q in QUERIES.items()}
    content=generate_email_content(data)
    tos=[st.session_state.get(f"email{i}") for i in range(1,6) if st.session_state.get(f"email{i}")]
    send_email("Îâ¥Ïä§ Î≥¥Îìú ÏöîÏïΩ",content,tos)

# -----------------------------
# Streamlit UI
# -----------------------------
def render_articles(comp: str, arts: List[Dict], cnt: int) -> None:
    st.subheader(f"üì∞ {comp} Îâ¥Ïä§")
    if not arts:
        st.info("ÏÉàÎ°úÏö¥ Îâ¥Ïä§Í∞Ä ÏóÜÏäµÎãàÎã§.")
        return
    for a in sorted(arts,key=lambda x: parse_datetime_naive(x["publishedAt"]) or datetime.min,reverse=True)[:cnt]:
        ts=parse_datetime_naive(a["publishedAt"]) or datetime.min
        ts=ts.strftime("%Y-%m-%d %H:%M")
        st.markdown(f"- [{_shorten(a['title'])}]({a['url']}) ({ts})")

def analyze_trends(src: str) -> None:
    st.markdown("---")
    st.header(f"üìà ÎÖ∏Ï∂ú Ïù¥Î†• (31Ïùº) ‚Äî {src}")
    cache=_load_cache()
    today=date.today()
    dates=[(today-timedelta(days=i)).strftime("%Y-%m-%d") for i in reversed(range(31))]
    cmap={d:{c:0 for c in KEYWORDS} for d in dates}
    for itm in cache.values():
        dt=parse_datetime_naive(itm["publishedAt"])
        if not dt: continue
        d0=dt.strftime("%Y-%m-%d")
        if d0 not in cmap: continue
        tl=itm["title"].lower()
        for comp,kws in KEYWORDS.items():
            if any(kw in tl for kw in kws):
                cmap[d0][comp]+=1
    rec=[{"date":d,"company":c,"count":cmap[d][c]} for d in dates for c in KEYWORDS]
    df=pd.DataFrame(rec)
    df["date_fmt"]=pd.to_datetime(df["date"]).dt.strftime("%m-%d")
    chart=(alt.Chart(df).mark_bar()
           .encode(x=alt.X("date_fmt:N",title="ÎÇ†Ïßú"),
                   y=alt.Y("count:Q",title="Í±¥Ïàò"),
                   color=alt.Color("company:N",title="ÌöåÏÇ¨"),
                   tooltip=["date_fmt","company","count"])
           .properties(width="container",height=300))
    st.altair_chart(chart,use_container_width=True)

def main() -> None:
    st.set_page_config(page_title="ÏóêÏä§ÏóîÏãúÏä§ Îâ¥Ïä§ Î≥¥Îìú",layout="wide")
    st.title("üìó ÏóêÏä§ÏóîÏãúÏä§ Îâ¥Ïä§ Î≥¥Îìú")

    # ‚îÄ ÏÇ¨Ïù¥ÎìúÎ∞î ‚îÄ
    src=st.sidebar.selectbox("Îâ¥Ïä§ ÏÜåÏä§",
        ["Ï†ÑÏ≤¥ (ÎÑ§Ïù¥Î≤Ñ Ìè¨Ìï®)","Ï†ÑÏ≤¥ (ÎÑ§Ïù¥Î≤Ñ Ï†úÏô∏)","RSSÎßå","NewsAPIÎßå"])
    use_nv="Ìè¨Ìï®" in src
    cnt=st.sidebar.selectbox("ÌëúÏãú Îâ¥Ïä§ Í±¥Ïàò",[5,10,12,15],index=1)

    st.sidebar.header("üìß Î©îÏùº ÏÑ§Ï†ï")
    t=st.sidebar.selectbox("Î©îÏùº Î∞úÏÜ° ÏãúÍ∞Ñ",["08:00","10:00","14:00","16:00"],key="send_time")
    for i in range(1,6):
        st.sidebar.text_input(f"Ïù¥Î©îÏùº {i}",key=f"email{i}")
    if st.sidebar.button("Î©îÏùº Ïä§ÏºÄÏ§Ñ Ï†ÄÏû•"):
        schedule.clear("emailjob")
        schedule.every().day.at(t).do(send_daily_email).tag("emailjob")
        st.sidebar.success("Ï†ÄÏû•ÎêòÏóàÏäµÎãàÎã§.")

    st.sidebar.header("ÎÖ∏Ï∂ú Ïù¥Î†• ÏÑ§Ï†ï")
    ts=st.sidebar.selectbox("ÏÜåÏä§ ÏÑ†ÌÉù",["Ï†ÑÏ≤¥","ÎÑ§Ïù¥Î≤ÑÎßå","Íµ¨Í∏Ä RSSÎßå"],index=0)

    # Ïä§ÏºÄÏ§ÑÎü¨
    if "sched" not in st.session_state:
        def _run():
            while True:
                schedule.run_pending(); time.sleep(1)
        threading.Thread(target=_run,daemon=True).start()
        st.session_state["sched"]=True

    if st.sidebar.button("üîÑ ÏÉàÎ°úÍ≥†Ïπ®"):
        st.cache_data.clear()
        st.rerun()

    # ‚îÄ ÌÉ≠Î≥Ñ Îâ¥Ïä§ ‚îÄ
    tabs=st.tabs(list(QUERIES.keys()))
    for tab,comp in zip(tabs,QUERIES):
        with tab:
            arts=fetch_news(QUERIES[comp],src,use_nv)
            render_articles(comp,arts,cnt)

    # ‚îÄ ÌÇ§ÏõåÎìú Î∂ÑÏÑù ‚îÄ
    st.markdown("---"); st.header("üîë Ï£ºÏöî ÌÇ§ÏõåÎìú Î∂ÑÏÑù")
    cols=st.columns(len(QUERIES))
    for col,comp in zip(cols,QUERIES):
        with col:
            arts=fetch_news(QUERIES[comp],src,use_nv)
            titles=[a["title"] for a in sorted(arts,key=lambda x: parse_datetime_naive(x["publishedAt"]) or datetime.min,reverse=True)[:cnt]]
            kws=extract_top_keywords(titles,5)
            if kws:
                for kw in kws: st.markdown(f"- {kw}")
            else:
                st.write("ÌÇ§ÏõåÎìú ÏóÜÏùå")

    # ‚îÄ ÎÖ∏Ï∂ú Ïù¥Î†• ‚îÄ
    analyze_trends(ts)

if __name__ == "__main__":
    main()
