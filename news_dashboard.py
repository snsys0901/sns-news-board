# -*- coding: utf-8 -*-
"""
ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ  (v2.6, 2025-05-27)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ BWMS / IAS / FGSS + ë™ì  2ì¢…(ì œí’ˆ1Â·ì œí’ˆ2) = 5ê°œ ì œí’ˆ ëª¨ë‹ˆí„°ë§
â€¢ í‚¤ì›Œë“œ ì¶”ì¶œ ë³´ê°• â†’ í•­ìƒ ìµœëŒ€ 5ê°œ, ì œëª© ë™ì¼ í† í° ì œì™¸
â€¢ ê·¸ë˜í”„ .interactive() ì ìš©(íœ  ì¤ŒÂ·ë“œë˜ê·¸ íŒ¬ ê°€ëŠ¥)
â€¢ ì‚¬ì´ë“œë°” ë¼ë²¨ ëª…í™•í™” (ì œí’ˆ1 / ì œí’ˆ2)
"""

# â”€â”€ ê³µí†µ import / ê²½ê³  ì–µì œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import os
import warnings
import logging
import re
import json
import time
import hashlib
from datetime import datetime, date, timedelta
from pathlib import Path
from collections import Counter
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor
from zoneinfo import ZoneInfo

import requests
import pandas as pd
import streamlit as st
import altair as alt
from bs4 import BeautifulSoup
from feedparser import parse as rss_parse
from sklearn.feature_extraction.text import TfidfVectorizer

os.environ["STREAMLIT_SUPPRESS_NO_SCRIPT_RUN_CONTEXT_WARNING"] = "true"
warnings.filterwarnings("ignore", message=".*ScriptRunContext.*")
logging.getLogger(
    "streamlit.runtime.scriptrunner.script_run_context"
).setLevel(logging.ERROR)

# â”€â”€ í˜ì´ì§€ & ìŠ¤íƒ€ì¼ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ë³´ë“œ", layout="wide")
st.markdown(
    """<script>try{localStorage.setItem('theme','light');}catch(e){}</script>""",
    unsafe_allow_html=True,
)
st.markdown(
    """
<style>
.stApp{background:#F8F9FA;font-family:'Segoe UI',sans-serif}
[data-testid="stHeader"]{background:#2C3E50!important}
[data-testid="stHeader"] h1{color:#fff!important}
[data-testid="stSidebar"]{background:#fff;border-right:1px solid #E1E4E8}
.stMetric{background:#fff;border:1px solid #E1E4E8;border-radius:8px;padding:1rem}
.stExpander{background:#fff;border-radius:8px;margin-bottom:1rem}
</style>""",
    unsafe_allow_html=True,
)

# â”€â”€ ë¡œê¹… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("NewsBoard")

# â”€â”€ ì „ì—­ ìƒìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CACHE_FILE = Path.home() / ".news_cache.json"
NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")

FIXED_QUERIES = {
    "ì—ìŠ¤ì—”ì‹œìŠ¤": "ì—ìŠ¤ì—”ì‹œìŠ¤ OR S&SYS",
    "ì‚¼ì„±ì¤‘ê³µì—…": "ì‚¼ì„±ì¤‘ê³µì—…",
    "í•œí™”ì˜¤ì…˜": "í•œí™”ì˜¤ì…˜",
}

STATIC_PRODUCTS = [
    ("BWMS", ["BWMS", "BWTS", "ì„ ë°•í‰í˜•ìˆ˜"]),
    ("IAS", [
        "IAS",
        "í†µí•©ìë™í™”ì‹œìŠ¤í…œ",
        "Integrated Automation System",
        "ì„ ë°•ìš© ì œì–´ì‹œìŠ¤í…œ",
        "ì„ ë°• ì œì–´ì‹œìŠ¤í…œ",
        "ì½©ìŠ¤ë²„ê·¸",
        "ì„ ë°•ìš© IAS",
    ]),
    ("FGSS", ["FGSS", "LFSS", "ì„ ë°•ì´ì¤‘ì—°ë£Œì‹œìŠ¤í…œ"]),
]

DEFAULT_P1_SYNS = ["ë°°ì „ë°˜", "ìˆ˜ë°°ì „ë°˜", "ì „ë ¥ë°°ì „ë°˜", "ì „ë ¥ê¸°ê¸°"]
DEFAULT_P2_SYNS = [
    "ì¹œí™˜ê²½", "ì¹œí™˜ê²½ ì„ ë°•", "ê·¸ë¦°ì‹­", "íƒˆíƒ„ì†Œ", "ì €íƒ„ì†Œ ì„ ë°•",
]

NOISE_WORDS = {
    "null", "ë‰´ìŠ¤", "ê¸°ì‚¬", "ì‚¬ì§„", "ìµœê·¼",
    *FIXED_QUERIES,
}

# â”€â”€ ìœ í‹¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _shorten(t: str, w: int = 60) -> str:
    return t if len(t) <= w else t[:w] + "â€¦"

def parse_datetime(s: str) -> Optional[datetime]:
    if not s:
        return None
    s = s.strip()
    now = datetime.now(ZoneInfo("Asia/Seoul"))
    for pat, unit in [(r"(\d+)ë¶„ ì „", "minutes"), (r"(\d+)ì‹œê°„ ì „", "hours")]:
        if m := re.match(pat, s):
            return now - timedelta(**{unit: int(m[1])})
    if s in ("ì˜¤ëŠ˜", "today"):
        return now
    if s in ("ì–´ì œ", "yesterday"):
        return now - timedelta(days=1)
    for fmt in ("%Y-%m-%d %H:%M", "%Y.%m.%d.", "%Y-%m-%d", "%Y-%m-%dT%H:%M:%SZ"):
        try:
            return datetime.strptime(s, fmt).replace(tzinfo=ZoneInfo("Asia/Seoul"))
        except ValueError:
            continue
    return None

def clean_text(t: str) -> str:
    t = re.sub(r"http\S+", "", t)
    t = re.sub(r"[^ê°€-í£A-Za-z0-9\s]", " ", t)
    return re.sub(r"\s+", " ", t).strip()

def _tfidf(texts: List[str], stop: set, top_n: int) -> List[str]:
    vect = TfidfVectorizer(
        token_pattern=r"(?u)\b[ê°€-í£A-Za-z0-9]{2,}\b",
        ngram_range=(1, 3),
        max_features=500,
    )
    try:
        X = vect.fit_transform(texts)
    except ValueError:
        return []
    scores = X.sum(axis=0).A1
    terms = vect.get_feature_names_out()
    josa = re.compile(r"(ìœ¼ë¡œ|ë¡œ|ì™€|ê³¼|ì´|ê°€|ì€|ëŠ”|ë„|ì„|ë¥¼)$")
    cand: List[str] = []
    for t, sc in sorted(zip(terms, scores), key=lambda x: -x[1]):
        tok = josa.sub("", t)
        if tok.lower() not in stop and not tok.isdigit():
            cand.append(tok)
        if len(cand) >= top_n:
            break
    return cand

def _freq_fallback(texts: List[str], stop: set, top_n: int) -> List[str]:
    freq = Counter()
    for line in texts:
        for tok in re.findall(r"[ê°€-í£A-Za-z0-9]{2,}", line):
            if tok.lower() not in stop and not tok.isdigit():
                freq[tok] += 1
    return [w for w, _ in freq.most_common(top_n)]

def extract_top_keywords(
    docs: List[str],
    top_n: int,
    extra_stop: set,
    fallback_word: str,
) -> List[str]:
    texts = [clean_text(d) for d in docs if d.strip()]
    stop = NOISE_WORDS | extra_stop
    if not texts:
        return [fallback_word]
    kws = _tfidf(texts, stop, top_n)
    if len(kws) < top_n:
        more = _freq_fallback(texts, stop, top_n * 2)
        for w in more:
            if w not in kws:
                kws.append(w)
            if len(kws) >= top_n:
                break
    kws = [k for k in kws if k.lower() != fallback_word.lower()][:top_n]
    return kws or [fallback_word]

def dedup(lst: List[Dict]) -> List[Dict]:
    seen, uniq = set(), []
    for a in lst:
        url = a.get("url")
        if url and url not in seen:
            uniq.append(a)
            seen.add(url)
    return uniq

# â”€â”€ ê¸°ì‚¬ ìˆ˜ì§‘ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=3600)
def fetch_newsapi(q: str) -> List[Dict]:
    if not NEWS_API_KEY:
        return []
    since = (datetime.utcnow() - timedelta(days=7)).strftime("%Y-%m-%dT%H:%M:%SZ")
    params = {
        "q": q,
        "language": "ko",
        "sortBy": "publishedAt",
        "from": since,
        "apiKey": NEWS_API_KEY,
        "pageSize": 100,
    }
    try:
        r = requests.get(
            "https://newsapi.org/v2/everything",
            params=params,
            headers={"User-Agent": "Mozilla/5.0"},
            timeout=10,
        )
        r.raise_for_status()
        arts = r.json().get("articles", [])
        for a in arts:
            a.setdefault("origins", []).append("newsapi")
            a["content"] = a.get("content", "") or ""
    except Exception:
        logger.exception("NewsAPI ì˜¤ë¥˜")
        arts = []
    return arts

@st.cache_data(ttl=3600)
def fetch_rss(q: str) -> List[Dict]:
    out, seen = [], set()
    for term in re.split(r"\s+OR\s+", q):
        url = (
            f"https://news.google.com/rss/search?"
            f"q={requests.utils.quote(term)}&hl=ko&gl=KR&ceid=KR:ko"
        )
        try:
            r = requests.get(url, timeout=10)
            r.raise_for_status()
            feed = rss_parse(r.text)
            for e in feed.entries:
                if e.link in seen:
                    continue
                seen.add(e.link)
                summary = BeautifulSoup(e.get("summary", ""), "html.parser").get_text()
                dt = (time.strftime("%Y-%m-%d %H:%M", e.published_parsed)
                    if getattr(e, "published_parsed", None) else "")
                out.append({
                    "title": e.title,
                    "url": e.link,
                    "publishedAt": dt,
                    "content": summary,
                    "origins": ["rss"],
                })
        except Exception:
            logger.warning(f"RSS ì˜¤ë¥˜: {url}")
    return out

@st.cache_data(ttl=3600)
def fetch_naver(q: str) -> List[Dict]:
    try:
        url = f"https://search.naver.com/search.naver?where=news&query={requests.utils.quote(q)}&sort=1"
        r = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, timeout=10)
        r.raise_for_status()
    except Exception:
        logger.exception("Naver ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜")
        return []
    soup = BeautifulSoup(r.text, "html.parser")
    out = []
    for it in soup.select("li.bx") + soup.select("div.news_area"):
        a_tag = it.select_one("a.news_tit")
        if not a_tag:
            continue
        title = a_tag.get("title") or a_tag.get_text(strip=True)
        link = a_tag["href"]
        dt_tag = it.select_one("span.date") or it.select_one("span.info")
        dt = dt_tag.get_text(strip=True) if dt_tag else ""
        desc = it.select_one("a.api_txt_lines") or it.select_one("div.news_dsc")
        content = desc.get_text(strip=True) if desc else ""
        out.append({
            "title": title,
            "url": link,
            "publishedAt": dt,
            "content": content,
            "origins": ["naver"],
        })
    return out

# â”€â”€ fetch_all & ìºì‹œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_all(q: str, mode: str, use_nv: bool) -> List[Dict]:
    funcs = []
    if mode != "RSSë§Œ":
        funcs.append(fetch_newsapi)
    if mode != "NewsAPIë§Œ":
        funcs.append(fetch_rss)
    if use_nv:
        funcs.append(fetch_naver)
    arts: List[Dict] = []
    with ThreadPoolExecutor() as ex:
        for fut in [ex.submit(fn, q) for fn in funcs]:
            arts.extend(fut.result())
    arts = dedup(arts)
    update_cache(arts)
    return arts

def update_cache(arts: List[Dict]) -> None:
    cache: Dict[str, Dict] = {}
    if CACHE_FILE.exists():
        try:
            cache = json.loads(CACHE_FILE.read_text("utf-8"))
        except json.JSONDecodeError:
            pass
    changed = False
    for a in arts:
        url = a.get("url", "")
        if not url:
            continue
        uid = hashlib.sha256(url.encode()).hexdigest()
        if uid not in cache:
            cache[uid] = a
            changed = True
    purge_before = datetime.now(ZoneInfo("Asia/Seoul")) - timedelta(days=30)
    for uid, a in list(cache.items()):
        dt = parse_datetime(a.get("publishedAt", ""))
        if dt and dt < purge_before:
            del cache[uid]
            changed = True
    if changed:
        CACHE_FILE.write_text(json.dumps(cache, ensure_ascii=False, indent=2), "utf-8")

# â”€â”€ ì¶”ì´ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def analyze_trends(
    arts: List[Dict], kw_map: Dict[str, List[str]], start: date, end: date
) -> pd.DataFrame:
    dates = pd.date_range(start, end)
    cmap = {d.strftime("%Y-%m-%d"): {c: 0 for c in kw_map} for d in dates}
    for it in arts:
        dt = parse_datetime(it.get("publishedAt", ""))
        if not dt:
            continue
        day = dt.strftime("%Y-%m-%d")
        if day not in cmap:
            continue
        txt = (it.get("title", "") + " " + it.get("content", "")).lower()
        for comp, kws in kw_map.items():
            if any(k.lower() in txt for k in kws):
                cmap[day][comp] += 1
    rows = [
        {"date": d, "company": c, "count": n}
        for d, v in cmap.items()
        for c, n in v.items()
    ]
    df = pd.DataFrame(rows)
    df["date_fmt"] = pd.to_datetime(df["date"]).dt.strftime("%m-%d")
    return df

# â”€â”€ Streamlit ë©”ì¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main() -> None:
    # â”€â”€ Sidebar ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # 1) ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ì„ í•„í„° ì„¤ì • ë°”ë¡œ ìœ„ë¡œ ì´ë™
    if st.sidebar.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()

    # 2) í•„í„° ì„¤ì • í—¤ë” ë° ìœ„ì ¯
    st.sidebar.header("í•„í„° ì„¤ì •")
    mode = st.sidebar.selectbox(
        "ë‰´ìŠ¤ ì†ŒìŠ¤",
        ("ì „ì²´ (ë„¤ì´ë²„ í¬í•¨)", "ì „ì²´ (ë„¤ì´ë²„ ì œì™¸)", "RSSë§Œ", "NewsAPIë§Œ"),
        0,
    )
    use_nv = "í¬í•¨" in mode
    cnt = st.sidebar.slider("ê¸°ì‚¬ í‘œì‹œ ê±´ìˆ˜", 5, 30, 10, 5)

    today = date.today()
    default_start = today - timedelta(days=30)
    start_date, end_date = st.sidebar.date_input(
        "ë¶„ì„ ê¸°ê°„", (default_start, today)
    )
    if start_date > end_date:
        st.sidebar.error("ì‹œì‘ì¼ì€ ì¢…ë£Œì¼ë³´ë‹¤ ì´ì „ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    st.sidebar.markdown("---")
    comp1 = st.sidebar.text_input("íšŒì‚¬1 (ë™ì )", "í•œë¼IMS")
    comp2 = st.sidebar.text_input("íšŒì‚¬2 (ë™ì )", "íŒŒë‚˜ì‹œì•„")
    st.sidebar.markdown("---")

    prod1_name = st.sidebar.text_input("ì œí’ˆ1 (ë™ì )", "ë°°ì „ë°˜")
    prod2_name = st.sidebar.text_input("ì œí’ˆ2 (ë™ì )", "ì¹œí™˜ê²½")

    # â”€â”€ ì œí’ˆ ì¿¼ë¦¬ êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def synonyms(name: str) -> List[str]:
        base = name.strip() or "ì œí’ˆ"
        if base == "ë°°ì „ë°˜":
            return DEFAULT_P1_SYNS
        if base == "ì¹œí™˜ê²½":
            return DEFAULT_P2_SYNS
        return [base]

    PRODUCT_QUERIES = STATIC_PRODUCTS + [
        (prod1_name.strip() or "ì œí’ˆ1", synonyms(prod1_name)),
        (prod2_name.strip() or "ì œí’ˆ2", synonyms(prod2_name)),
    ]

    # â”€â”€ Title & Metrics â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.title("ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ë³´ë“œ")
    comp_list = list(FIXED_QUERIES) + [comp1, comp2]
    cols = st.columns(len(comp_list))
    data_map: Dict[str, List[Dict]] = {}

    with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘â€¦"):
        for i, comp in enumerate(comp_list):
            arts = [
                a
                for a in fetch_all(FIXED_QUERIES.get(comp, comp), mode, use_nv)
                if (
                    (dt := parse_datetime(a.get("publishedAt", "")))
                    and start_date <= dt.date() <= end_date
                )
            ]
            data_map[comp] = arts
            cols[i].metric(f"{comp} ê¸°ì‚¬ ìˆ˜", len(arts))

    st.markdown("---")

    # â”€â”€ 1) ì—…ì²´ë³„ ìµœì‹  ë‰´ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    for tab, comp in zip(st.tabs(data_map.keys()), data_map):
        with tab:
            st.subheader(f"{comp} ìµœì‹  ë‰´ìŠ¤ (ìƒìœ„ {cnt}ê±´)")
            subset = sorted(
                data_map[comp],
                key=lambda x: parse_datetime(x["publishedAt"]) or datetime.min,
                reverse=True,
            )[:cnt]
            if not subset:
                st.info("í˜„ì¬ ì¡°íšŒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            for a in subset:
                ts = parse_datetime(a["publishedAt"])
                ts_str = ts.strftime("%Y-%m-%d %H:%M") if ts else ""
                st.markdown(
                    f"- [{_shorten(a['title'])}]({a['url']}) "
                    f"<span style='color:#6B7280;'>({ts_str})</span>",
                    unsafe_allow_html=True,
                )

    st.markdown("---")

    # â”€â”€ 2) ì œí’ˆë³„ ìµœì‹  ë‰´ìŠ¤ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("ì œí’ˆë³„ ìµœì‹  ë‰´ìŠ¤")
    for tab, (title, syns) in zip(
        st.tabs([t for t, _ in PRODUCT_QUERIES]), PRODUCT_QUERIES
    ):
        with tab:
            st.subheader(f"{title} ìµœì‹  ë‰´ìŠ¤ (ìƒìœ„ {cnt}ê±´)")
            q = " OR ".join(syns)
            arts = sorted(
                fetch_all(q, mode, use_nv),
                key=lambda x: parse_datetime(x.get("publishedAt", "")) or datetime.min,
                reverse=True,
            )[:cnt]
            if not arts:
                st.info("í˜„ì¬ ì¡°íšŒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            for a in arts:
                ts = parse_datetime(a["publishedAt"])
                ts_str = ts.strftime("%Y-%m-%d %H:%M") if ts else ""
                st.markdown(
                    f"- [{_shorten(a['title'])}]({a['url']}) "
                    f"<span style='color:#6B7280;'>({ts_str})</span>",
                    unsafe_allow_html=True,
                )

    st.markdown("---")

    # â”€â”€ 3) ê¸°ì—…ë³„ í‚¤ì›Œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (ìƒìœ„ 5ê°œ)", True):
        kcols = st.columns(len(data_map))
        for col, comp in zip(kcols, data_map):
            texts = [
                a["title"] + " " + a.get("content", "")
                for a in data_map[comp][: cnt * 3]
            ]
            kws = extract_top_keywords(texts, 5, {comp.lower()}, comp)
            col.markdown(f"**{comp}**")
            for w in kws:
                col.write(f"- {w}")

    # â”€â”€ 4) ì œí’ˆë³„ í‚¤ì›Œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with st.expander("ğŸ”‘ ì œí’ˆë³„ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (ìƒìœ„ 5ê°œ)", False):
        pcols = st.columns(len(PRODUCT_QUERIES))
        for col, (title, syns) in zip(pcols, PRODUCT_QUERIES):
            q = " OR ".join(syns)
            arts = [
                a
                for a in fetch_all(q, mode, use_nv)
                if (
                    (dt := parse_datetime(a.get("publishedAt", "")))
                    and start_date <= dt.date() <= end_date
                )
            ][: cnt * 3]
            texts = [a["title"] + " " + a.get("content", "") for a in arts]
            kws = extract_top_keywords(texts, 5, {s.lower() for s in syns}, title)
            col.markdown(f"**{title}**")
            for w in kws:
                col.write(f"- {w}")

    st.markdown("---")

    # â”€â”€ 5) ë…¸ì¶œ ì¶”ì´ ë¶„ì„ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("ë…¸ì¶œ ì¶”ì´ ë¶„ì„")
    trend_df = analyze_trends(
        sum(data_map.values(), []),
        {**{k: [k] for k in FIXED_QUERIES}, comp1: [comp1], comp2: [comp2]},
        start_date,
        end_date,
    )
    selected = [
        comp
        for comp in comp_list
        if st.sidebar.checkbox(comp, True, key=f"cb_{comp}")
    ]
    trend_df = trend_df[trend_df["company"].isin(selected)]
    chart = (
        alt.Chart(trend_df)
        .mark_line(point=True)
        .encode(
            x=alt.X("date_fmt:O", title="ë‚ ì§œ"),
            y=alt.Y("count:Q", title="ê±´ìˆ˜"),
            color=alt.Color("company:N", title="íšŒì‚¬"),
            tooltip=["date_fmt", "company", "count"],
        )
        .properties(width="container", height=400)
        .interactive()
    )
    st.altair_chart(chart, use_container_width=True)

if __name__ == "__main__":
    main()
