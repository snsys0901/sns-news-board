# -*- coding: utf-8 -*-
"""
ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ  (v2.7.7, 2025-06-26)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ UX ìµœì í™”: ì¹´ë“œ ë ˆì´ì•„ì›ƒ, ë©€í‹°ì»¬ëŸ¼, í—¤ë”ì— ë¡œê³  ë°±ê·¸ë¼ìš´ë“œ ë°©ì‹ ì‚½ì…
â€¢ ëª¨ë“ˆí™”: ë Œë”ë§ í•¨ìˆ˜ ë¶„ë¦¬
â€¢ ì‚¬ì´ë“œë°” ê¸°ê°„Â·ì†ŒìŠ¤Â·ê±´ìˆ˜Â·ë™ì  í•„í„° ê°•í™”
â€¢ ë²„ê·¸ ìˆ˜ì •: ìºì‹œ ì—…ë°ì´íŠ¸ Windows ê¶Œí•œ ë¬¸ì œ í•´ê²°, ì°¨íŠ¸ ì„ íƒ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ì²˜ë¦¬, ì œí’ˆ íƒ­ ë Œë”ë§ ë°©ì‹ ìˆ˜ì •
â€¢ ê¸°ëŠ¥ ì¶”ê°€: í™”ë©´ ìƒë‹¨ í—¤ë” ë°°ê²½ì— ë¡œê³  ì‚½ì… (ì™¼ìª½)
â€¢ ê¸°ëŠ¥ ì¶”ê°€: í—¤ë” í•˜ë‹¨ì— í‘¸ë¥¸ìƒ‰ ê·¸ë¼ë°ì´ì…˜ í…Œë‘ë¦¬ ì ìš©
"""

import os
import warnings
import logging
import re
import json
import time
import hashlib
import tempfile
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor
from zoneinfo import ZoneInfo

import requests
import pandas as pd
import streamlit as st
import altair as alt
from bs4 import BeautifulSoup
from feedparser import parse as rss_parse
from sklearn.feature_extraction.text import TfidfVectorizer

# â”€â”€ í™˜ê²½ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
os.environ["STREAMLIT_SUPPRESS_NO_SCRIPT_RUN_CONTEXT_WARNING"] = "true"
warnings.filterwarnings("ignore", message=".*ScriptRunContext.*")
logging.getLogger("streamlit.runtime.scriptrunner.script_run_context").setLevel(logging.ERROR)
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger("NewsBoard")

# â”€â”€ í˜ì´ì§€ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ë³´ë“œ", layout="wide")
st.markdown(
    """<script>try{localStorage.setItem('theme','light');}catch(e){};</script>""",
    unsafe_allow_html=True,
)

# â”€â”€ í—¤ë” ë¡œê³  ë°°ê²½ ì‚½ì… ë° ê·¸ë¼ë°ì´ì…˜ í…Œë‘ë¦¬ ì ìš© â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
LOGO_URL = "https://pds.saramin.co.kr/company/logo/201903/20/pon8bu_ngqk-ya4cjo_logo.jpg"
st.markdown(
    f"""
    <style>
    [data-testid="stHeader"] {{
        /* ë°°ê²½ì€ ìˆœìˆ˜ í°ìƒ‰ + ë¡œê³  ì´ë¯¸ì§€ */
        background-color: #ffffff;
        background-image: url('{LOGO_URL}');
        background-repeat: no-repeat;
        background-position: left center;
        background-size: auto 60px;
        padding-left: 100px;

        /* ê·¸ë¼ë°ì´ì…˜ í…Œë‘ë¦¬ */
        border-bottom: 6px solid transparent;
        border-image-source: linear-gradient(to right, #1E90FF, #00BFFF);
        border-image-slice: 1;
    }}
    [data-testid="stHeader"] h1 {{
        /* í° ë°”íƒ• ëŒ€ë¹„ ì˜ ë³´ì´ë„ë¡ ë‹¤í¬ ê·¸ë ˆì´ë¡œ ë³€ê²½ */
        color: #333 !important;
        margin-left: 100px;
    }}
    </style>
    """,
    unsafe_allow_html=True
)

# â”€â”€ ì „ì—­ ìŠ¤íƒ€ì¼ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.markdown(
    """
    <style>
    .stApp { background: #F8F9FA; font-family: 'Segoe UI', sans-serif; }
    [data-testid="stSidebar"] { background: #fff; border-right: 1px solid #E1E4E8; }
    .stMetric { background: #fff; border: 1px solid #E1E4E8; border-radius: 8px; padding: 1rem; }
    .news-card { background: #fff; border: 1px solid #E1E4E8; border-radius: 8px; padding: 1rem; margin-bottom: 1rem; }
    .news-card a { color: #2C3E50; text-decoration: none; font-weight: bold; }
    .news-card .timestamp { color: #6B7280; font-size: 0.8rem; margin-top: 0.5rem; }
    .news-card .desc { margin-top: 0.5rem; font-size: 0.9rem; }
    </style>
    """,
    unsafe_allow_html=True,
)

# â”€â”€ ìƒìˆ˜ ì •ì˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CACHE_FILE = Path.home() / ".news_cache.json"
NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")

FIXED_QUERIES = {
    "ì—ìŠ¤ì—”ì‹œìŠ¤": "ì—ìŠ¤ì—”ì‹œìŠ¤ OR S&SYS",
    "ì‚¼ì„±ì¤‘ê³µì—…": "ì‚¼ì„±ì¤‘ê³µì—…",
    "í•œí™”ì˜¤ì…˜": "í•œí™”ì˜¤ì…˜",
}
STATIC_PRODUCTS = [
    ("BWMS", ["BWMS", "BWTS", "ì„ ë°•í‰í˜•ìˆ˜"]),
    ("IAS", ["IAS","í†µí•©ìë™í™”ì‹œìŠ¤í…œ","Integrated Automation System","ì„ ë°•ìš© ì œì–´ì‹œìŠ¤í…œ","ì„ ë°• ì œì–´ì‹œìŠ¤í…œ","ì½©ìŠ¤ë²„ê·¸","ì„ ë°•ìš© IAS"]),
    ("FGSS", ["FGSS","LFSS","ì„ ë°•ì´ì¤‘ì—°ë£Œì‹œìŠ¤í…œ"]),
]
DEFAULT_P1_SYNS = ["ë°°ì „ë°˜", "ìˆ˜ë°°ì „ë°˜", "ì „ë ¥ë°°ì „ë°˜", "ì „ë ¥ê¸°ê¸°"]
DEFAULT_P2_SYNS = ["ì¹œí™˜ê²½", "ì¹œí™˜ê²½ ì„ ë°•", "ê·¸ë¦°ì‹­", "íƒˆíƒ„ì†Œ", "ì €íƒ„ì†Œ ì„ ë°•"]
NOISE_WORDS = {"null","ë‰´ìŠ¤","ê¸°ì‚¬","ì‚¬ì§„","ìµœê·¼", *FIXED_QUERIES}

# â”€â”€ ìœ í‹¸ í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def _shorten(text: str, width: int = 80) -> str:
    return text if len(text) <= width else text[:width] + "â€¦"

def parse_datetime(s: str) -> Optional[datetime]:
    if not s: return None
    now = datetime.now(ZoneInfo("Asia/Seoul"))
    for pat, unit in [(r"(\d+)ë¶„ ì „", "minutes"), (r"(\d+)ì‹œê°„ ì „", "hours")]:
        m = re.match(pat, s)
        if m: return now - timedelta(**{unit: int(m.group(1))})
    if s in ("ì˜¤ëŠ˜", "today"): return now
    if s in ("ì–´ì œ", "yesterday"): return now - timedelta(days=1)
    for fmt in ("%Y-%m-%d %H:%M", "%Y.%m.%d.", "%Y-%m-%d", "%Y-%m-%dT%H:%M:%SZ"):
        try:
            return datetime.strptime(s, fmt).replace(tzinfo=ZoneInfo("Asia/Seoul"))
        except ValueError:
            continue
    return None

def clean_text(t: str) -> str:
    t = re.sub(r"http\S+", "", t)
    t = re.sub(r"[^ê°€-í£A-Za-z0-9\s]", " ", t)
    return re.sub(r"\s+", " ", t).strip()

def extract_top_keywords(docs: List[str], top_n: int, extra_stop: set, fallback: str) -> List[str]:
    texts = [clean_text(d) for d in docs if d.strip()]
    stop = NOISE_WORDS | extra_stop
    if not texts: return [fallback]
    vect = TfidfVectorizer(token_pattern=r"(?u)\b[ê°€-í£A-Za-z0-9]{2,}\b", ngram_range=(1,3), max_features=500)
    try:
        X = vect.fit_transform(texts)
    except ValueError:
        return [fallback]
    scores = X.sum(axis=0).A1; terms = vect.get_feature_names_out()
    ranked = sorted(zip(scores, terms), reverse=True)
    clean_terms = [re.sub(r"(ìœ¼ë¡œ|ë¡œ|ì™€|ê³¼|ì´|ê°€|ì€|ëŠ”|ë„|ì„|ë¥¼)$", "", t) for _, t in ranked]
    filtered = [w for w in clean_terms if w.lower() not in stop and not w.isdigit()]
    return [w for w in filtered if w.lower() != fallback.lower()][:top_n] or [fallback]

def dedup(items: List[Dict]) -> List[Dict]:
    seen, unique = set(), []
    for a in items:
        u = a.get("url")
        if u and u not in seen:
            unique.append(a); seen.add(u)
    return unique
# â”€â”€ ë°ì´í„° ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data(ttl=3600)
def fetch_newsapi(q: str) -> List[Dict]:
    arts=[]
    if NEWS_API_KEY:
        since=(datetime.utcnow()-timedelta(days=7)).strftime("%Y-%m-%dT%H:%M:%SZ")
        params={"q":q,"language":"ko","sortBy":"publishedAt","from":since,"apiKey":NEWS_API_KEY,"pageSize":100}
        try:
            r=requests.get("https://newsapi.org/v2/everything", params=params, timeout=10)
            r.raise_for_status()
            for a in r.json().get("articles", []):
                a.setdefault("origins",[]).append("newsapi")
                a["content"]=a.get("content", "") or ""
                arts.append(a)
        except Exception:
            logger.exception("NewsAPI ì˜¤ë¥˜")
    return arts

@st.cache_data(ttl=3600)
def fetch_rss(q: str) -> List[Dict]:
    out, seen = [], set()
    for term in q.split(" OR "):
        url = f"https://news.google.com/rss/search?q={requests.utils.quote(term)}&hl=ko&gl=KR&ceid=KR:ko"
        try:
            r=requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
            r.raise_for_status()
            feed=rss_parse(r.text)
            for e in feed.entries:
                if e.link in seen: continue
                seen.add(e.link)
                summary=BeautifulSoup(e.get("summary",""),"html.parser").get_text()
                dt=time.strftime("%Y-%m-%d %H:%M", e.published_parsed) if getattr(e,"published_parsed", None) else ""
                out.append({"title":e.title,"url":e.link,"publishedAt":dt,"content":summary,"origins":["rss"]})
        except Exception:
            logger.warning(f"RSS ì˜¤ë¥˜: {url}")
    return out

@st.cache_data(ttl=3600)
def fetch_naver(q: str) -> List[Dict]:
    out=[]
    try:
        url=f"https://search.naver.com/search.naver?where=news&query={requests.utils.quote(q)}&sort=1"
        r=requests.get(url, headers={"User-Agent":"Mozilla/5.0"}, timeout=10)
        r.raise_for_status()
    except Exception:
        logger.exception("Naver ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜")
        return out
    soup=BeautifulSoup(r.text,"html.parser")
    for it in soup.select("li.bx") + soup.select("div.news_area"):
        a_tag=it.select_one("a.news_tit")
        if not a_tag: continue
        title=a_tag.get("title") or a_tag.get_text(strip=True)
        link=a_tag["href"]
        dt_tag=it.select_one("span.date") or it.select_one("span.info")
        dt=dt_tag.get_text(strip=True) if dt_tag else ""
        desc=it.select_one("a.api_txt_lines") or it.select_one("div.news_dsc")
        content=desc.get_text(strip=True) if desc else ""
        out.append({"title":title,"url":link,"publishedAt":dt,"content":content,"origins":["naver"]})
    return out


def fetch_all(q: str, mode: str, use_nv: bool) -> List[Dict]:
    funcs=[]
    if mode != "RSSë§Œ": funcs.append(fetch_newsapi)
    if mode != "NewsAPIë§Œ": funcs.append(fetch_rss)
    if use_nv: funcs.append(fetch_naver)
    arts=[]
    with ThreadPoolExecutor() as ex:
        for fut in [ex.submit(fn, q) for fn in funcs]:
            try: arts.extend(fut.result())
            except Exception as e: logger.warning(f"fetch error: {e}")
    arts=dedup(arts)
    update_cache(arts)
    return arts


def update_cache(arts: List[Dict]) -> None:
    cache={}
    if CACHE_FILE.exists():
        try: cache=json.loads(CACHE_FILE.read_text(encoding="utf-8"))
        except:
            try: CACHE_FILE.unlink(missing_ok=True)
            except: pass
            cache={}
    changed=False
    for a in arts:
        url=a.get("url","")
        if not url: continue
        uid=hashlib.sha256(url.encode()).hexdigest()
        if uid not in cache: cache[uid]=a; changed=True
    purge_before=datetime.now(ZoneInfo("Asia/Seoul"))-timedelta(days=30)
    for uid,a in list(cache.items()):
        dt=parse_datetime(a.get("publishedAt",""))
        if dt and dt<purge_before: del cache[uid]; changed=True
    if changed:
        try:
            tmp=tempfile.NamedTemporaryFile("w", delete=False, dir=CACHE_FILE.parent, encoding="utf-8")
            json.dump(cache, tmp, ensure_ascii=False, indent=2)
            tmp.flush(); tmp.close()
            if CACHE_FILE.exists(): CACHE_FILE.unlink()
            os.rename(tmp.name, CACHE_FILE)
        except Exception as e:
            logger.warning(f"Cache update failed: {e}")


def analyze_trends(arts: List[Dict], kw_map: Dict[str, List[str]], start: date, end: date) -> pd.DataFrame:
    dates=pd.date_range(start, end)
    cmap={d.strftime("%Y-%m-%d"): {k: 0 for k in kw_map} for d in dates}
    for a in arts:
        dt=parse_datetime(a.get("publishedAt",""))
        if not dt: continue
        day=dt.strftime("%Y-%m-%d")
        if day not in cmap: continue
        txt=(a.get("title","")+" "+a.get("content","")).lower()
        for comp,kws in kw_map.items():
            if any(kw.lower() in txt for kw in kws): cmap[day][comp]+=1
    rows=[{"date":d,"company":c,"count":n} for d,v in cmap.items() for c,n in v.items()]
    df=pd.DataFrame(rows)
    df["date_fmt"]=pd.to_datetime(df["date"]).dt.strftime("%m-%d")
    return df


def display_article_cards(articles: List[Dict], cols_per_row: int = 3):
    for i in range(0, len(articles), cols_per_row):
        batch=articles[i:i+cols_per_row]
        cols=st.columns(len(batch))
        for col,art in zip(cols,batch):
            ts=parse_datetime(art.get("publishedAt",""))
            ts_str=ts.strftime("%Y-%m-%d %H:%M") if ts else ""
            desc=(art.get("content","")[:100]+"â€¦") if art.get("content") else ""
            html=(
                f"<div class='news-card'>"
                f"<a href=\"{art['url']}\" target=\"_blank\">{_shorten(art['title'])}</a>"
                f"<div class='timestamp'>{ts_str}</div>"
                f"<div class='desc'>{desc}</div>"
                "</div>"
            )
            with col: st.markdown(html, unsafe_allow_html=True)


def main():
    if st.sidebar.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"): st.cache_data.clear()
    st.sidebar.header("í•„í„° ì„¤ì •")
    mode=st.sidebar.selectbox("ë‰´ìŠ¤ ì†ŒìŠ¤", ("ì „ì²´ (ë„¤ì´ë²„ í¬í•¨)", "ì „ì²´ (ë„¤ì´ë²„ ì œì™¸)", "RSSë§Œ", "NewsAPIë§Œ"), 0)
    use_nv="í¬í•¨" in mode
    cnt=st.sidebar.slider("ê¸°ì‚¬ í‘œì‹œ ê±´ìˆ˜", 5, 30, 10, 5)
    today=date.today()
    dates=st.sidebar.date_input("ë¶„ì„ ê¸°ê°„", (today-timedelta(days=30), today))
    if isinstance(dates, tuple): start_date,end_date=dates
    else: start_date=end_date=dates
    if start_date>end_date: st.sidebar.error("ì‹œì‘ì¼ì€ ì¢…ë£Œì¼ë³´ë‹¤ ì´ì „ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    comp1=st.sidebar.text_input("íšŒì‚¬1 (ë™ì )", "í•œë¼IMS")
    comp2=st.sidebar.text_input("íšŒì‚¬2 (ë™ì )", "íŒŒë‚˜ì‹œì•„")
    prod1=st.sidebar.text_input("ì œí’ˆ1 (ë™ì )", "ë°°ì „ë°˜")
    prod2=st.sidebar.text_input("ì œí’ˆ2 (ë™ì )", "ì¹œí™˜ê²½")

    st.title("ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ë³´ë“œ")
    comp_list=list(FIXED_QUERIES)+[comp1,comp2]
    cols_metrics=st.columns(len(comp_list))
    data_map={}
    with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘â€¦"):
        for i,comp in enumerate(comp_list):
            arts=[a for a in fetch_all(FIXED_QUERIES.get(comp,comp),mode,use_nv)
                  if (dt:=parse_datetime(a.get("publishedAt",""))) and start_date<=dt.date()<=end_date]
            data_map[comp]=arts
            cols_metrics[i].metric(f"{comp} ê¸°ì‚¬ ìˆ˜", len(arts))

    st.markdown("---")
    for tab,comp in zip(st.tabs(list(data_map.keys())), data_map):
        with tab:
            st.subheader(f"{comp} ìµœì‹  ë‰´ìŠ¤ (ìƒìœ„ {cnt}ê±´)")
            subset=sorted(data_map[comp], key=lambda x: parse_datetime(x.get("publishedAt","")) or datetime.min, reverse=True)[:cnt]
            if not subset: st.info("í˜„ì¬ ì¡°íšŒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else: display_article_cards(subset)

    st.markdown("---")
    st.subheader("ì œí’ˆë³„ ìµœì‹  ë‰´ìŠ¤")
    prod_tabs = st.tabs([t for t,_ in STATIC_PRODUCTS] + [prod1 or "ì œí’ˆ1", prod2 or "ì œí’ˆ2"] )
    prod_queries = STATIC_PRODUCTS + [
        (prod1 or "ì œí’ˆ1", DEFAULT_P1_SYNS if prod1=="ë°°ì „ë°˜" else [prod1]),
        (prod2 or "ì œí’ˆ2", DEFAULT_P2_SYNS if prod2=="ì¹œí™˜ê²½" else [prod2]),
    ]
    for tab, (title, syns) in zip(prod_tabs, prod_queries):
        with tab:
            st.subheader(f"{title} ìµœì‹  ë‰´ìŠ¤ (ìƒìœ„ {cnt}ê±´)")
            arts=sorted(fetch_all(" OR ".join(syns),mode,use_nv),
                        key=lambda x: parse_datetime(x.get("publishedAt","")) or datetime.min, reverse=True)[:cnt]
            if not arts: st.info("í˜„ì¬ ì¡°íšŒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            else: display_article_cards(arts)

    st.markdown("---")
    with st.expander("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (íšŒì‚¬ë³„ ìƒìœ„ 5ê°œ)", True):
        cols_kw=st.columns(len(data_map))
        for col, comp in zip(cols_kw, data_map):
            texts=[a["title"]+a.get("content","") for a in data_map[comp][:cnt*3]]
            kws=extract_top_keywords(texts,5,{comp.lower()},comp)
            col.markdown(f"**{comp}**")
            for w in kws: col.write(f"- {w}")

    st.markdown("---")
    with st.expander("ğŸ”‘ ì œí’ˆë³„ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (ìƒìœ„ 5ê°œ)", False):
        cols_kwp=st.columns(len(prod_queries))
        for col, (title, syns) in zip(cols_kwp, prod_queries):
            texts=[a["title"]+a.get("content","") for a in sorted(fetch_all(" OR ".join(syns),mode,use_nv),
                        key=lambda x: parse_datetime(x.get("publishedAt","")) or datetime.min)][:cnt*3]
            kws=extract_top_keywords(texts,5,{s.lower() for s in syns},title)
            col.markdown(f"**{title}**")
            for w in kws: col.write(f"- {w}")

    st.markdown("---")
    trend_df=analyze_trends(sum(data_map.values(),[]), {**{k:[k] for k in FIXED_QUERIES}, comp1:[comp1], comp2:[comp2]}, start_date, end_date)
    selected=[c for c in comp_list if st.sidebar.checkbox(c, True, key=f"cb_{c}")]
    if not selected: selected=comp_list
    chart=(
        alt.Chart(trend_df[trend_df["company"].isin(selected)])
        .mark_line(point=True)
        .encode(
            x=alt.X("date_fmt:O", title="ë‚ ì§œ"),
            y=alt.Y("count:Q", title="ê±´ìˆ˜"),
            color=alt.Color("company:N", title="íšŒì‚¬"),
            tooltip=["date_fmt","company","count"],
        )
        .properties(width="container", height=400)
        .interactive()
    )
    st.altair_chart(chart, use_container_width=True)

if __name__=="__main__": main()
