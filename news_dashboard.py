# -*- coding: utf-8 -*-
"""
ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ (ëª¨ë˜ UI/UX) â€“ v2.2  
â€¢ BWMS í‚¤ì›Œë“œ ëˆ„ë½ í•´ê²° â†’ ì˜ë¬¸Â·ìˆ«ì í† í° í—ˆìš©(2â€“15ì) & ë¹ˆ TFâ€‘IDF ì•ˆì „ ì²˜ë¦¬  
â€¢ â€˜NULLâ€™ í† í°Â·ë¶ˆìš©ì–´ ì¶”ê°€ í•„í„°ë§, í‚¤ì›Œë“œ ì—†ì„ ë•Œ â€˜â€“â€™ ì¶œë ¥  
â€¢ ThreadPool ê²½ê³  ì™„ì „ ì–µì œ(log í•„í„°)  
â€¢ ê¸°íƒ€ ì†Œì†Œí•œ ë¦¬íŒ©í„°ë§
"""

from __future__ import annotations

# ------------------------------------------------------
# ê²½ê³  ì–µì œ ì„¤ì • (Streamlit ë¡œë“œ ì „ì—!)
# ------------------------------------------------------
import os, logging, warnings
os.environ["STREAMLIT_SUPPRESS_NO_SCRIPT_RUN_CONTEXT_WARNING"] = "true"  # ë‚´ë¶€ ì˜µì…˜
warnings.filterwarnings("ignore", message=".*ScriptRunContext.*")  # fallback
logging.getLogger("streamlit.runtime.scriptrunner.script_run_context").setLevel(logging.ERROR)

# í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import re, json, time, hashlib
from datetime import datetime, date, timedelta
from pathlib import Path
from typing import List, Dict, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed
from zoneinfo import ZoneInfo

# ì™¸ë¶€ ë¼ì´ë¸ŒëŸ¬ë¦¬
import requests, pandas as pd, altair as alt, streamlit as st
from bs4 import BeautifulSoup
from feedparser import parse as rss_parse
from sklearn.feature_extraction.text import TfidfVectorizer

# ------------------------------------------------------
# í˜ì´ì§€ ì„¤ì •
# ------------------------------------------------------
st.set_page_config(
    page_title="ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ë³´ë“œ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# ê¸°ë³¸ í…Œë§ˆ ìŠ¤í¬ë¦½íŠ¸
st.markdown(
    """
    <script>try{if(!localStorage.getItem('theme'))localStorage.setItem('theme','light');}catch(e){}</script>
    """,
    unsafe_allow_html=True,
)

# ì»¤ìŠ¤í…€ CSS
st.markdown(
    """
    <style>
      .stApp{background:#F8F9FA;font-family:'Segoe UI',sans-serif}
      [data-testid="stHeader"]{background:#2C3E50!important}
      [data-testid="stHeader"] h1{color:#fff!important}
      [data-testid="stSidebar"]{background:#fff;border-right:1px solid #E1E4E8}
      .stMetric{background:#fff;border:1px solid #E1E4E8;border-radius:8px;padding:1rem}
      .stExpander{background:#fff;border-radius:8px;margin-bottom:1rem}
    </style>
    """,
    unsafe_allow_html=True,
)

# ------------------------------------------------------
# ë¡œê¹…
# ------------------------------------------------------
logging.basicConfig(level=logging.INFO,
                    format="%(asctime)s [%(levelname)s] %(message)s",
                    datefmt="%Y-%m-%d %H:%M:%S")
logger = logging.getLogger("NewsBoard")

# ------------------------------------------------------
# ì „ì—­ ìƒìˆ˜
# ------------------------------------------------------
CACHE_FILE = Path.home() / ".news_cache.json"
CACHE_MAX_DAYS = 30
NEWS_API_KEY = os.getenv("NEWS_API_KEY", "")

FIXED_QUERIES: Dict[str, str] = {
    "ì—ìŠ¤ì—”ì‹œìŠ¤": "ì—ìŠ¤ì—”ì‹œìŠ¤ OR S&SYS",
    "ì‚¼ì„±ì¤‘ê³µì—…": "ì‚¼ì„±ì¤‘ê³µì—…",
    "í•œí™”ì˜¤ì…˜": "í•œí™”ì˜¤ì…˜",
}

PRODUCT_QUERIES: List[tuple[str, List[str]]] = [
    ("BWMS", ["BWMS", "BWTS", "ì„ ë°•í‰í˜•ìˆ˜"]),
    ("IAS", ["ì„ ë°•ìš© ì œì–´ì‹œìŠ¤í…œ", "ì„ ë°• ì œì–´ì‹œìŠ¤í…œ", "ì½©ìŠ¤ë²„ê·¸", "ì„ ë°•ìš© IAS"]),
    ("FGSS", ["FGSS", "LFSS", "ì„ ë°•ì´ì¤‘ì—°ë£Œì‹œìŠ¤í…œ"]),
]

# ë¶ˆìš©ì–´(ê²€ìƒ‰ì–´Â·íšŒì‚¬/ì œí’ˆëª…Â·ì¼ë°˜ ë‹¨ì–´)
NOISE_WORDS: set[str] = {
    *(kw.lower() for kw in FIXED_QUERIES),
    *[w.lower() for _, syn in PRODUCT_QUERIES for w in syn],
    "null", "ë‰´ìŠ¤", "ê¸°ì‚¬", "ìµœê·¼", "ì‚¬ì§„", "ì œê³µ", "ëŒ€í•œ", "ê´€ë ¨"
}

# ------------------------------------------------------
# ìœ í‹¸ í•¨ìˆ˜
# ------------------------------------------------------

def _shorten(txt: str, width: int = 60) -> str:
    return txt if len(txt) <= width else txt[: width] + "â€¦"


def parse_datetime(s: str) -> Optional[datetime]:
    """ë‹¤ì–‘í•œ ë‚ ì§œ ë¬¸ìì—´ â†’ KST tz-aware datetime"""
    if not s:
        return None
    s = s.strip()
    now = datetime.now(ZoneInfo("Asia/Seoul"))
    # ìƒëŒ€í‘œê¸°
    rel_patterns = {
        r"(\d+)ë¶„ ì „": lambda m: now - timedelta(minutes=int(m[1])),
        r"(\d+)ì‹œê°„ ì „": lambda m: now - timedelta(hours=int(m[1])),
        r"(\d+)ì¼ ì „": lambda m: now - timedelta(days=int(m[1])),
    }
    for pat, func in rel_patterns.items():
        if (m := re.match(pat, s)):
            return func(m)
    if s in ("ì–´ì œ", "í•˜ë£¨ ì „"):
        return now - timedelta(days=1)
    if s == "ì˜¤ëŠ˜":
        return now
    # ì ˆëŒ€í‘œê¸°
    for fmt in (
        "%Y-%m-%d %H:%M",
        "%Y.%m.%d.",
        "%Y-%m-%dT%H:%M:%SZ",
        "%Y-%m-%d",
        "%a, %d %b %Y %H:%M:%S %z",
    ):
        try:
            dt = datetime.strptime(s, fmt)
            dt = dt.replace(tzinfo=ZoneInfo("Asia/Seoul")) if dt.tzinfo is None else dt.astimezone(ZoneInfo("Asia/Seoul"))
            return dt
        except ValueError:
            continue
    return None


def clean_text(t: str) -> str:
    t = re.sub(r"http\S+", "", t)
    t = re.sub(r"[^ê°€-í£A-Za-z0-9\s]", " ", t)
    return re.sub(r"\s+", " ", t).strip()


def extract_top_keywords(docs: List[str], top_n: int = 5, exclude: set[str] | None = None) -> List[str]:
    texts = [clean_text(d) for d in docs if d.strip()]
    if not texts:
        return []
    try:
        vect = TfidfVectorizer(
            token_pattern=r"(?u)\b[ê°€-í£A-Za-z0-9]{2,15}\b",
            ngram_range=(1, 3),
            max_features=500,
        )
        X = vect.fit_transform(texts)
    except ValueError:  # ë¹ˆ ì–´íœ˜
        return []
    scores = X.sum(axis=0).A1
    terms = vect.get_feature_names_out()
    josa = re.compile(r"(ìœ¼ë¡œ|ë¡œ|ì™€|ê³¼|ì´|ê°€|ì€|ëŠ”|ë„)$")
    exclude_low = {w.lower() for w in (exclude or set())} | NOISE_WORDS
    result: List[str] = []
    for term, score in sorted(zip(terms, scores), key=lambda x: -x[1]):
        ct = josa.sub("", term)
        if 2 <= len(ct) <= 15 and ct.lower() not in exclude_low:
            result.append(ct)
        if len(result) >= top_n:
            break
    return result

# ------------------------------------------------------
# ë„¤íŠ¸ì›Œí¬ ìš”ì²­ ìœ í‹¸
# ------------------------------------------------------

def _request(url: str, **kwargs):
    for _ in range(3):
        try:
            r = requests.get(url, timeout=10, headers={"User-Agent": "Mozilla/5.0"}, **kwargs)
            r.raise_for_status()
            return r
        except Exception as e:
            logger.warning(f"{url[:60]} â€¦ ì¬ì‹œë„ ({e})")
            time.sleep(1)
    raise RuntimeError(f"ìš”ì²­ ì‹¤íŒ¨: {url}")

# ------------------------------------------------------
# ê¸°ì‚¬ ìˆ˜ì§‘ í•¨ìˆ˜
# ------------------------------------------------------
@st.cache_data(ttl=3600)
def fetch_newsapi(q: str) -> List[Dict]:
    if not NEWS_API_KEY:
        return []
    since = (datetime.utcnow() - timedelta(days=7)).strftime("%Y-%m-%dT%H:%M:%SZ")
    params = {
        "q": q,
        "language": "ko",
        "sortBy": "publishedAt",
        "from": since,
        "apiKey": NEWS_API_KEY,
        "pageSize": 100,
    }
    try:
        arts = _request("https://newsapi.org/v2/everything", params=params).json().get("articles", [])
        for a in arts:
            a.setdefault("origins", []).append("newsapi")
            a["content"] = a.get("content", "") or ""
        return arts
    except Exception:
        logger.exception("NewsAPI ì˜¤ë¥˜")
        return []

@st.cache_data(ttl=3600)
def fetch_rss(q: str) -> List[Dict]:
    out, seen = [], set()
    for term in re.split(r"\s+OR\s+", q):
        url = f"https://news.google.com/rss/search?q={requests.utils.quote(term)}&hl=ko&gl=KR&ceid=KR:ko"
        try:
            feed = rss_parse(_request(url).text)
            for e in feed.entries:
                if e.link in seen:
                    continue
                seen.add(e.link)
                dt = time.strftime("%Y-%m-%d %H:%M", e.published_parsed) if getattr(e, "published_parsed", None) else ""
                out.append(
                    {
                        "title": e.title,
                        "url": e.link,
                        "publishedAt": dt,
                        "content": BeautifulSoup(e.get("summary", ""), "html.parser").get_text(),
                        "origins": ["rss"],
                    }
                )
        except Exception:
            logger.warning(f"RSS ì˜¤ë¥˜: {url}")
    return out

@st.cache_data(ttl=3600)
def fetch_naver(q: str) -> List[Dict]:
    out = []
    try:
        url = f"https://search.naver.com/search.naver?where=news&query={requests.utils.quote(q)}&sort=1"
        soup = BeautifulSoup(_request(url).text, "html.parser")
        items = soup.select("li.bx") + soup.select("div.news_area")
        for it in items:
            a_tag = it.select_one("a.news_tit")
            if not a_tag:
                continue
            title = a_tag.get("title") or a_tag.get_text(strip=True)
            link = a_tag["href"]
            dt_tag = it.select_one("span.date") or it.select_one("span.info")
            dt = dt_tag.get_text(strip=True) if dt_tag else ""
            desc = it.select_one("a.api_txt_lines") or it.select_one("div.news_dsc")
            content = desc.get_text(strip=True) if desc else ""
            out.append(
                {
                    "title": title,
                    "url": link,
                    "publishedAt": dt,
                    "content": content,
                    "origins": ["naver"],
                }
            )
    except Exception:
        logger.exception("Naver ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜")
    terms = [t.lower() for t in re.split(r"\s+OR\s+", q) if t.strip()]
    return [a for a in out if any(t in (a["title"] + " " + a["content"]).lower() for t in terms)]

# ------------------------------------------------------
# ìºì‹œ ë° ì¤‘ë³µ ê´€ë¦¬
# ------------------------------------------------------

def _purge_old(cache: Dict[str, Dict]) -> Dict[str, Dict]:
    threshold = datetime.now(ZoneInfo("Asia/Seoul")) - timedelta(days=CACHE_MAX_DAYS)
    return {uid: a for uid, a in cache.items() if (parse_datetime(a.get("publishedAt", "")) or threshold) >= threshold}


def update_cache(arts: List[Dict]) -> None:
    cache: Dict[str, Dict] = {}
    if CACHE_FILE.exists():
        try:
            cache = json.loads(CACHE_FILE.read_text("utf-8"))
        except json.JSONDecodeError:
            pass
    cache = _purge_old(cache)
    changed = False
    for a in arts:
        url = a.get("url", "")
        if not url:
            continue
        uid = hashlib.sha256(url.encode()).hexdigest()
        if uid not in cache:
            cache[uid] = a
            changed = True
    if changed:
        CACHE_FILE.write_text(json.dumps(cache, ensure_ascii=False, indent=2), "utf-8")


def dedup(arts: List[Dict]) -> List[Dict]:
    seen, uniq = set(), []
    for a in arts:
        url = a.get("url")
        if url and url not in seen:
            uniq.append(a)
            seen.add(url)
    return uniq

# ------------------------------------------------------
# í†µí•© ìˆ˜ì§‘
# ------------------------------------------------------

def fetch_all(q: str, mode: str, use_nv: bool) -> List[Dict]:
    tasks = []
    if mode != "RSSë§Œ":
        tasks.append(("newsapi", lambda: fetch_newsapi(q)))
    if mode != "NewsAPIë§Œ":
        tasks.append(("rss", lambda: fetch_rss(q)))
    if use_nv:
        tasks.append(("naver", lambda: fetch_naver(q)))

    arts = []
    with ThreadPoolExecutor() as ex:
        futures = {ex.submit(fn): name for name, fn in tasks}
        for fut in as_completed(futures):
            try:
                arts.extend(fut.result())
            except Exception:
                logger.exception(f"{futures[fut]} ìˆ˜ì§‘ ì‹¤íŒ¨")
    arts = dedup(arts)
    update_cache(arts)
    return arts

# ------------------------------------------------------
# ì¶”ì´ ë¶„ì„
# ------------------------------------------------------

def analyze_trends(arts: List[Dict], kw_map: Dict[str, List[str]], start: date, end: date) -> pd.DataFrame:
    dates = pd.date_range(start, end)
    cmap = {d.strftime("%Y-%m-%d"): {c: 0 for c in kw_map} for d in dates}
    for it in arts:
        dt = parse_datetime(it.get("publishedAt", ""))
        if not dt:
            continue
        d = dt.strftime("%Y-%m-%d")
        if d not in cmap:
            continue
        txt = (it.get("title", "") + " " + it.get("content", "")).lower()
        for comp, kws in kw_map.items():
            if any(k.lower() in txt for k in kws):
                cmap[d][comp] += 1
    rows = [{"date": d, "company": c, "count": n} for d, v in cmap.items() for c, n in v.items()]
    df = pd.DataFrame(rows)
    df["date_fmt"] = pd.to_datetime(df["date"]).dt.strftime("%m-%d")
    return df

# ------------------------------------------------------
# ë©”ì¸ UI
# ------------------------------------------------------

def main():
    # Sidebar
    st.sidebar.header("í•„í„° ì„¤ì •")
    mode = st.sidebar.selectbox("ë‰´ìŠ¤ ì†ŒìŠ¤", ["ì „ì²´ (ë„¤ì´ë²„ í¬í•¨)", "ì „ì²´ (ë„¤ì´ë²„ ì œì™¸)", "RSSë§Œ", "NewsAPIë§Œ"], 0)
    use_nv = "í¬í•¨" in mode
    cnt = st.sidebar.slider("ê¸°ì‚¬ í‘œì‹œ ê±´ìˆ˜", 5, 30, 10, 5)
    today = date.today(); default_start = today - timedelta(days=30)
    start_date, end_date = st.sidebar.date_input("ë¶„ì„ ê¸°ê°„", [default_start, today])
    if start_date > end_date:
        st.sidebar.error("ì‹œì‘ì¼ì€ ì¢…ë£Œì¼ ì´ì „ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")
    st.sidebar.markdown("---")
    comp1 = st.sidebar.text_input("íšŒì‚¬1 (ë™ì )", "í•œë¼IMS")
    comp2 = st.sidebar.text_input("íšŒì‚¬2 (ë™ì )", "íŒŒë‚˜ì‹œì•„")
    st.sidebar.markdown("---")
    all_comps = list(FIXED_QUERIES) + [comp1, comp2]
    selected = [c for c in all_comps if st.sidebar.checkbox(c, True)]
    if st.sidebar.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨"):
        st.cache_data.clear()
    if not NEWS_API_KEY:
        st.sidebar.warning("NEWS_API_KEY ë¯¸ì„¤ì • â€“ NewsAPI ê¸°ì‚¬ ì œì™¸")

    # Title & metrics
    st.title("ì—ìŠ¤ì—”ì‹œìŠ¤ ë‰´ìŠ¤ ë³´ë“œ")
    cols = st.columns(len(FIXED_QUERIES) + 2)
    data_map: Dict[str, List[Dict]] = {}
    with st.spinner("ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘..."):
        for i, comp in enumerate(list(FIXED_QUERIES) + [comp1, comp2]):
            arts = [
                a for a in fetch_all(FIXED_QUERIES.get(comp, comp), mode, use_nv)
                if (dt := parse_datetime(a.get("publishedAt", ""))) and start_date <= dt.date() <= end_date
            ]
            data_map[comp] = arts
            cols[i].metric(f"{comp} ê¸°ì‚¬ ìˆ˜", len(arts))

    st.markdown("---")

    # ì—…ì²´ë³„ ìµœì‹  ë‰´ìŠ¤
    tabs = st.tabs(list(data_map.keys()))
    for tab, comp in zip(tabs, data_map):
        with tab:
            st.subheader(f"{comp} ìµœì‹  ë‰´ìŠ¤ (ìƒìœ„ {cnt}ê±´)")
            subset = sorted(data_map[comp], key=lambda x: parse_datetime(x["publishedAt"]) or datetime.min, reverse=True)[:cnt]
            if not subset:
                st.info("í˜„ì¬ ì¡°íšŒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            for a in subset:
                ts = parse_datetime(a["publishedAt"]); ts_str = ts.strftime("%Y-%m-%d %H:%M") if ts else ""
                st.markdown(f"- [{_shorten(a['title'])}]({a['url']}) <span style='color:#6B7280;'>({ts_str})</span>", unsafe_allow_html=True)

    st.markdown("---")

    # ì œí’ˆë³„ ìµœì‹  ë‰´ìŠ¤
    st.subheader("ì œí’ˆë³„ ìµœì‹  ë‰´ìŠ¤")
    p_tabs = st.tabs([t for t, _ in PRODUCT_QUERIES])
    for tab, (title, syns) in zip(p_tabs, PRODUCT_QUERIES):
        with tab:
            st.subheader(f"{title} ìµœì‹  ë‰´ìŠ¤ (ìƒìœ„ {cnt}ê±´)")
            q = " OR ".join(syns)
            arts_display = sorted(
                fetch_all(q, mode, use_nv),
                key=lambda x: parse_datetime(x.get("publishedAt", "")) or datetime.min,
                reverse=True,
            )[:cnt]
            if not arts_display:
                st.info("í˜„ì¬ ì¡°íšŒí•  ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
            for a in arts_display:
                ts = parse_datetime(a["publishedAt"]); ts_str = ts.strftime("%Y-%m-%d %H:%M") if ts else ""
                st.markdown(f"- [{_shorten(a['title'])}]({a['url']}) <span style='color:#6B7280;'>({ts_str})</span>", unsafe_allow_html=True)

    st.markdown("---")

    # ê¸°ì—…ë³„ í‚¤ì›Œë“œ
    with st.expander("ğŸ”‘ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (ìƒìœ„ 5ê°œ)", True):
        kcols = st.columns(len(data_map))
        for col, comp in zip(kcols, data_map):
            texts = [a["title"] + " " + a.get("content", "") for a in data_map[comp][:30]]
            kws = extract_top_keywords(texts, 5, {comp})
            col.markdown(f"**{comp}**")
            if kws:
                for w in kws:
                    col.write(f"- {w}")
            else:
                col.write("â€“")

    # ì œí’ˆë³„ í‚¤ì›Œë“œ
    with st.expander("ğŸ”‘ ì œí’ˆë³„ ì£¼ìš” í‚¤ì›Œë“œ ë¶„ì„ (ìƒìœ„ 5ê°œ)", False):
        pcols = st.columns(len(PRODUCT_QUERIES))
        for col, (title, syns) in zip(pcols, PRODUCT_QUERIES):
            q = " OR ".join(syns)
            arts = [
                a for a in fetch_all(q, mode, use_nv)
                if (dt := parse_datetime(a.get("publishedAt", ""))) and start_date <= dt.date() <= end_date
            ][:30]
            texts = [a["title"] + " " + a.get("content", "") for a in arts]
            kws = extract_top_keywords(texts, 5, set(s.lower() for s in syns))
            col.markdown(f"**{title}**")
            if kws:
                for w in kws:
                    col.write(f"- {w}")
            else:
                col.write("â€“")

    st.markdown("---")

    # ë…¸ì¶œ ì¶”ì´
    st.subheader("ë…¸ì¶œ ì¶”ì´ ë¶„ì„")
    df = analyze_trends(
        sum(data_map.values(), []),
        {**{k: [k] for k in FIXED_QUERIES}, comp1: [comp1], comp2: [comp2]},
        start_date,
        end_date,
    )
    df = df[df["company"].isin(selected)]
    chart = (
        alt.Chart(df)
        .mark_line(point=True)
        .encode(
            x=alt.X("date_fmt:O", title="ë‚ ì§œ"),
            y=alt.Y("count:Q", title="ê±´ìˆ˜"),
            color=alt.Color("company:N", title="íšŒì‚¬"),
            tooltip=["date_fmt", "company", "count"],
        )
        .properties(width="container", height=400)
        .interactive()
    )
    st.altair_chart(chart, use_container_width=True)


if __name__ == "__main__":
    main()
